{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElahehJafarigol/Federated-Learning/blob/Research-paper-1/1_Federated_Base_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e58c1bIO21P"
      },
      "source": [
        "####**Deep Imbalanced Learning for Weather Data: A Federated Learning Approach**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eUCQ7bczFKkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q1py7JjP3ah",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install --quiet keras==2.9.0\n",
        "!pip install --quiet tensorflow==2.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obQ8Fi137uwW"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet imbalanced-learn\n",
        "!pip install --quiet git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwYDaHxwCR4Z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from  IPython import display\n",
        "import pathlib\n",
        "import shutil\n",
        "import tempfile\n",
        "import warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.filterwarnings('ignore')\n",
        "from pandas.core.common import random_state\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import datasets, layers, models, metrics\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras import backend as K\n",
        "from keras.layers import GaussianNoise\n",
        "from keras import regularizers\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import ReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers.activation import ReLU\n",
        "from keras.layers.serialization import activation\n",
        "from pandas.core.indexes.datetimes import Resolution\n",
        "from keras.layers import Concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "random_state = 42\n",
        "random_seed = 42\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import tensorflow_docs.plots as tfplots\n",
        "\n",
        "# Set the font to Times New Roman\n",
        "plt.rcParams['font.family'] = 'serif'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_data(client_num):\n",
        "    \"\"\"\n",
        "    Loads and processes data from Google Drive for a specific client.\n",
        "\n",
        "    Args:\n",
        "        client_num (int): The client number.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame, pd.DataFrame: Processed feature and label DataFrames.\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    file_path = f'/content/drive/MyDrive/Colab_Notebooks/Federated_Imbalanced_Learning/Stations/Federated_Learning/Client{client_num}.csv'\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Encoding 'RainToday' and 'RainTomorrow' as binary\n",
        "    df['RainToday'] = df['RainToday'].apply(lambda x: 1 if x == \"Yes\" else 0)\n",
        "    df['RainTomorrow'] = df['RainTomorrow'].apply(lambda x: 1 if x == \"Yes\" else 0)\n",
        "\n",
        "    # Dropping specific columns\n",
        "    df = df.drop(['Sunshine','Evaporation','Cloud3pm','Cloud9am','RISK_MM','Location','Date','WindGustDir',\n",
        "                  'WindDir9am', 'WindDir3pm'], axis=1)\n",
        "\n",
        "    # Replacing NaN values with respective means\n",
        "    fill_feat = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed','WindSpeed9am', 'WindSpeed3pm',\n",
        "                 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm','Temp9am', 'Temp3pm',\n",
        "                 'RainToday', 'RainTomorrow']\n",
        "    for i in fill_feat:\n",
        "        df[i].fillna(df[i].mean(), inplace=True)\n",
        "\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Separate the features and labels\n",
        "    X = df.drop('RainTomorrow', axis=1)\n",
        "    y = df['RainTomorrow']\n",
        "\n",
        "    # Normalize the features\n",
        "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
        "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    # Reset the index to ensure consistency\n",
        "    X.reset_index(drop=True, inplace=True)\n",
        "    y.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def calculate_class_ratio(y):\n",
        "    # Calculate the counts for minority and majority classes\n",
        "    minority_count = sum(y == 1)\n",
        "    majority_count = sum(y == 0)\n",
        "\n",
        "    # Calculate the imbalance ratio\n",
        "    class_ratio = minority_count / len(y)\n",
        "\n",
        "    # Print the results\n",
        "    print(\"Minority class:\", minority_count)\n",
        "    print(\"Majority class:\", majority_count)\n",
        "    print(\"Imbalance ratio:\", class_ratio)\n",
        "\n",
        "    return class_ratio\n",
        "\n",
        "def split_client_data(X, y, test_size=0.2, val_size=0.1):\n",
        "    \"\"\"\n",
        "    Splits data into training, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): Features DataFrame.\n",
        "        y (pd.DataFrame): Labels DataFrame.\n",
        "        test_size (float): Proportion of the data to include in the test set.\n",
        "        val_size (float): Proportion of the data to include in the validation set.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing the split data.\n",
        "    \"\"\"\n",
        "    # First, split into train + val and test\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    # Next, split train + val into train and val sets\n",
        "    val_actual_size = val_size / (1 - test_size)  # Adjust the validation size based on the initial split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_actual_size, random_state=42)\n",
        "\n",
        "    return {\n",
        "        \"X_train\": X_train,\n",
        "        \"y_train\": y_train,\n",
        "        \"X_val\": X_val,\n",
        "        \"y_val\": y_val,\n",
        "        \"X_test\": X_test,\n",
        "        \"y_test\": y_test\n",
        "    }\n",
        "\n",
        "# Process data for all clients and store in a dictionary\n",
        "all_client_data = {}\n",
        "clients = range(1, 10)\n",
        "\n",
        "for client_num in clients:\n",
        "    X, y = load_and_process_data(client_num)\n",
        "    all_client_data[client_num] = {\n",
        "        f\"X_{client_num}\": X,\n",
        "        f\"y_{client_num}\": y\n",
        "    }\n",
        "\n",
        "# Split data for all clients and store the results in a new dictionary\n",
        "split_data = {}\n",
        "\n",
        "for client_num, data in all_client_data.items():\n",
        "    X = data[f\"X_{client_num}\"]\n",
        "    y = data[f\"y_{client_num}\"]\n",
        "    split_data[client_num] = split_client_data(X, y)"
      ],
      "metadata": {
        "id": "73IMIh8xH59Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size\n",
        "Batch_size = 64  # example batch size\n",
        "\n",
        "def batch_data(data_shard, batchsize=Batch_size):\n",
        "    # unpack data shard into data and labels arrays\n",
        "    X, y = data_shard\n",
        "\n",
        "    # create a tensorflow dataset object from the data and labels\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "    # shuffle and batch the dataset\n",
        "    return dataset.shuffle(len(y)).batch(batchsize)\n",
        "\n",
        "# Batch the training data for all clients and store the results in a new dictionary\n",
        "batched_train_data = {}\n",
        "\n",
        "for client_num, data_splits in split_data.items():\n",
        "    X_train = data_splits[\"X_train\"]\n",
        "    y_train = data_splits[\"y_train\"]\n",
        "    batched_train_data[client_num] = batch_data((X_train, y_train), Batch_size)\n",
        "\n",
        "# Example of accessing the batched training dataset for all clients\n",
        "for client_number in range(1, 10):\n",
        "    train_dataset = batched_train_data[client_number]\n",
        "\n",
        "\n",
        "# Batch the validation data for all clients and store the results in a new dictionary\n",
        "batched_validation_data = {}\n",
        "\n",
        "for client_num, data_splits in split_data.items():\n",
        "    X_val = data_splits[\"X_val\"]\n",
        "    y_val = data_splits[\"y_val\"]\n",
        "    batched_validation_data[client_num] = batch_data((X_val, y_val), Batch_size)\n",
        "\n",
        "# Example of accessing the batched training dataset for all clients\n",
        "for client_number in range(1, 10):\n",
        "    validation_dataset = batched_validation_data[client_number]"
      ],
      "metadata": {
        "id": "D38TiP6GLwTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the test datasets from all clients\n",
        "combined_X_test = pd.concat([split_data[client][\"X_test\"] for client in split_data])\n",
        "combined_y_test = pd.concat([split_data[client][\"y_test\"] for client in split_data])\n",
        "\n",
        "print(\"Test data shape:\", combined_X_test.shape)\n",
        "print(\"Test data labels shape:\", combined_y_test.shape)\n",
        "\n",
        "# Process and batch the test set\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((combined_X_test, combined_y_test)).batch(len(combined_y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvTZHN-cP4nx",
        "outputId": "7bcf0ed5-dddb-4b1f-de94-d82c0e42c26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data shape: (28139, 13)\n",
            "Test data labels shape: (28139,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Federated Averaging: aggregation method\n",
        "def weight_scalling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    #get the batch_size\n",
        "    batch_size = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*batch_size\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*batch_size\n",
        "    return local_count/global_count\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    # Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "\n",
        "    return avg_grad\n",
        "\n",
        "def test_model(X_test, Y_test, model, communication_round):\n",
        "    cce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    logits = model.predict(X_test, batch_size=Batch_size)\n",
        "    logits = tf.squeeze(logits, axis=1) # remove last dimension from logits\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.round(logits), Y_test)\n",
        "    auc = roc_auc_score(Y_test, logits)\n",
        "    tn, fp, fn, tp = confusion_matrix(Y_test, tf.round(logits)).ravel()\n",
        "    g_mean = np.sqrt(tp/(tp+fn)*tn/(tn+fp))\n",
        "    print('communication_round: {} | global_accuracy: {:.3%} | global_loss: {} | global_AUC: {:.3%} | global_G-mean: {:.3%}'.format(communication_round, acc, loss, auc, g_mean))\n",
        "    return acc, loss, auc, g_mean"
      ],
      "metadata": {
        "id": "KRKKjDi0lxqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define data shape and number of classes\n",
        "input_shape = (13,)\n",
        "data_shape = (13,)\n",
        "num_classes = 2\n",
        "Batch_size = 64\n",
        "Learning_rate = 0.001\n",
        "Momentum = 0.9\n",
        "Epochs = 30\n",
        "stddev = 0.01\n",
        "communication_rounds = 10\n",
        "\n",
        "epochs = Epochs\n",
        "learning_rate = Learning_rate\n",
        "momentum = Momentum\n",
        "batch_size = Batch_size\n",
        "\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = keras.optimizers.SGD(learning_rate=learning_rate,\n",
        "                                 momentum = momentum,\n",
        "                                 nesterov = False)\n",
        "loss = \"binary_crossentropy\"\n",
        "\n",
        "metrics = [\n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'),\n",
        "      ]\n",
        "\n",
        "class MyModel:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = tf.keras.Sequential([\n",
        "        layers.Dense(256, activation='relu', input_shape=(13,)),\n",
        "        layers.GaussianNoise(stddev),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "yP2eI2uFlxnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Federated Training with Local Validation:**"
      ],
      "metadata": {
        "id": "38HtWQCcwo19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smlp_global = MyModel()\n",
        "global_model = smlp_global.build(shape=data_shape, classes=num_classes)\n",
        "global_model.summary()\n",
        "\n",
        "# Initialize lists to store global metrics\n",
        "global_acc_list = []\n",
        "global_loss_list = []\n",
        "global_auc_list = []\n",
        "global_gmean_list = []\n",
        "\n",
        "# Commence global training loop\n",
        "for communication_round in range(communication_rounds):  # 'communication_round' is likely intended to be 'communication_rounds'\n",
        "\n",
        "    # Get the global model's weights to initialize local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    # Initialize list to collect scaled local model weights\n",
        "    scaled_local_weight_list = []\n",
        "\n",
        "    # Randomize client data\n",
        "    client_names = list(batched_train_data.keys())\n",
        "    random.shuffle(client_names)\n",
        "\n",
        "    # Loop through each client and train local models\n",
        "    for client in client_names:\n",
        "        smlp_local = MyModel()\n",
        "        local_model = smlp_local.build(shape=data_shape, classes=num_classes)\n",
        "        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "        # Set local model weights to global model weights\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        # Fit local model with client's data\n",
        "        model_history = local_model.fit(batched_train_data[client], epochs=epochs, verbose=0)\n",
        "\n",
        "        # Validate the local model on associated validation set\n",
        "        X_val = split_data[client][\"X_val\"]\n",
        "        y_val = split_data[client][\"y_val\"]\n",
        "        val_metrics = local_model.evaluate(X_val, y_val, verbose=0)\n",
        "\n",
        "        val_loss, val_accuracy, val_auc, val_gmean = val_metrics[:4]\n",
        "\n",
        "        print(f'Local model {client} - val_loss: {val_loss} - val_accuracy: {val_accuracy} - val_auc: {val_auc} - val_gmean: {val_gmean}')\n",
        "\n",
        "        # Scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(batched_train_data, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        # Clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    # Compute the average of scaled weights\n",
        "    avg_grad = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    # Update global model weights\n",
        "    global_model.set_weights(avg_grad)\n",
        "\n",
        "    # Test global model and print metrics after each communication round\n",
        "    for X_test, y_test in test_batched:\n",
        "        global_acc, global_loss, global_auc, global_gmean = test_model(X_test, y_test, global_model, communication_round)\n",
        "        global_acc_list.append(global_acc)\n",
        "        global_loss_list.append(global_loss)\n",
        "        global_auc_list.append(global_auc)\n",
        "        global_gmean_list.append(global_gmean)\n"
      ],
      "metadata": {
        "id": "5sSlb7RIQZPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}